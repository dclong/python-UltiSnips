def collect(): Array[T]

Returns an array that contains all rows in this Dataset.
def collectAsList(): List[T]

Returns a Java list that contains all rows in this Dataset.
def count(): Long

Returns the number of rows in the Dataset.
def describe(cols: String*): DataFrame

Computes statistics for numeric and string columns, including count, mean, stddev, min, and max.
def first(): T

Returns the first row.
def foreach(func: ForeachFunction[T]): Unit

(Java-specific) Runs func on each element of this Dataset.
def foreach(f: (T) ⇒ Unit): Unit

Applies a function f to all rows.
def foreachPartition(func: ForeachPartitionFunction[T]): Unit

(Java-specific) Runs func on each partition of this Dataset.
def foreachPartition(f: (Iterator[T]) ⇒ Unit): Unit

Applies a function f to each partition of this Dataset.
def head(): T

Returns the first row.
def head(n: Int): Array[T]

Returns the first n rows.
Experimental def reduce(func: ReduceFunction[T]): T

(Java-specific) Reduces the elements of this Dataset using the specified binary function.
Experimental def reduce(func: (T, T) ⇒ T): T

(Scala-specific) Reduces the elements of this Dataset using the specified binary function.
def show(numRows: Int, truncate: Int): Unit

Displays the Dataset in a tabular form.
def show(numRows: Int, truncate: Boolean): Unit

Displays the Dataset in a tabular form.
def show(truncate: Boolean): Unit

Displays the top 20 rows of Dataset in a tabular form.
def show(): Unit

Displays the top 20 rows of Dataset in a tabular form.
def show(numRows: Int): Unit

Displays the Dataset in a tabular form.
def take(n: Int): Array[T]

Returns the first n rows in the Dataset.
def takeAsList(n: Int): List[T]

Returns the first n rows in the Dataset as a list.
def toLocalIterator(): Iterator[T]

Return an iterator that contains all rows in this Dataset.

Basic Dataset functions

Experimental def as[U](implicit arg0: Encoder[U]): Dataset[U]

Returns a new Dataset where each record has been mapped on to the specified type.
def cache(): Dataset.this.type

Persist this Dataset with the default storage level (MEMORY_AND_DISK).
Experimental def checkpoint(eager: Boolean): Dataset[T]

Returns a checkpointed version of this Dataset.
Experimental def checkpoint(): Dataset[T]

Eagerly checkpoint a Dataset and return the new Dataset.
def columns: Array[String]

Returns all column names as an array.
def createGlobalTempView(viewName: String): Unit

Creates a global temporary view using the given name.
def createOrReplaceGlobalTempView(viewName: String): Unit

Creates or replaces a global temporary view using the given name.
def createOrReplaceTempView(viewName: String): Unit

Creates a local temporary view using the given name.
def createTempView(viewName: String): Unit

Creates a local temporary view using the given name.
def dtypes: Array[(String, String)]

Returns all column names and their data types as an array.
def explain(): Unit

Prints the physical plan to the console for debugging purposes.
def explain(extended: Boolean): Unit

Prints the plans (logical and physical) to the console for debugging purposes.
def hint(name: String, parameters: Any*): Dataset[T]

Specifies some hint on the current Dataset.
def inputFiles: Array[String]

Returns a best-effort snapshot of the files that compose this Dataset.
def isLocal: Boolean

Returns true if the collect and take methods can be run locally (without any Spark executors).
def javaRDD: JavaRDD[T]

Returns the content of the Dataset as a JavaRDD of Ts.
def persist(newLevel: StorageLevel): Dataset.this.type

Persist this Dataset with the given storage level.
def persist(): Dataset.this.type

Persist this Dataset with the default storage level (MEMORY_AND_DISK).
def printSchema(): Unit

Prints the schema to the console in a nice tree format.
lazy val rdd: RDD[T]

Represents the content of the Dataset as an RDD of T.
def schema: StructType

Returns the schema of this Dataset.
def storageLevel: StorageLevel

Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
def toDF(colNames: String*): DataFrame

Converts this strongly typed collection of data to generic DataFrame with columns renamed.
def toDF(): DataFrame

Converts this strongly typed collection of data to generic Dataframe.
def toJavaRDD: JavaRDD[T]

Returns the content of the Dataset as a JavaRDD of Ts.
def unpersist(): Dataset.this.type

Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
def unpersist(blocking: Boolean): Dataset.this.type

Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
def write: DataFrameWriter[T]

Interface for saving the content of the non-streaming Dataset out into external storage.
def writeStream: DataStreamWriter[T]

Interface for saving the content of the streaming Dataset out into external storage.
def registerTempTable(tableName: String): Unit

Registers this Dataset as a temporary table using the given name.

streaming

Experimental def isStreaming: Boolean

Returns true if this Dataset contains one or more sources that continuously return data as it arrives.
Experimental def withWatermark(eventTime: String, delayThreshold: String): Dataset[T]

Defines an event time watermark for this Dataset.

Typed transformations

def alias(alias: Symbol): Dataset[T]

(Scala-specific) Returns a new Dataset with an alias set.
def alias(alias: String): Dataset[T]

Returns a new Dataset with an alias set.
def as(alias: Symbol): Dataset[T]

(Scala-specific) Returns a new Dataset with an alias set.
def as(alias: String): Dataset[T]

Returns a new Dataset with an alias set.
def coalesce(numPartitions: Int): Dataset[T]

Returns a new Dataset that has exactly numPartitions partitions, when the fewer partitions are requested.
def distinct(): Dataset[T]

Returns a new Dataset that contains only the unique rows from this Dataset.
def dropDuplicates(col1: String, cols: String*): Dataset[T]

Returns a new Dataset with duplicate rows removed, considering only the subset of columns.
def dropDuplicates(colNames: Array[String]): Dataset[T]

Returns a new Dataset with duplicate rows removed, considering only the subset of columns.
def dropDuplicates(colNames: Seq[String]): Dataset[T]

(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the subset of columns.
def dropDuplicates(): Dataset[T]

Returns a new Dataset that contains only the unique rows from this Dataset.
def except(other: Dataset[T]): Dataset[T]

Returns a new Dataset containing rows in this Dataset but not in another Dataset.
Experimental def filter(func: FilterFunction[T]): Dataset[T]

(Java-specific) Returns a new Dataset that only contains elements where func returns true.
Experimental def filter(func: (T) ⇒ Boolean): Dataset[T]

(Scala-specific) Returns a new Dataset that only contains elements where func returns true.
def filter(conditionExpr: String): Dataset[T]

Filters rows using the given SQL expression.
def filter(condition: Column): Dataset[T]

Filters rows using the given condition.
Experimental def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U]

(Java-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results.
Experimental def flatMap[U](func: (T) ⇒ TraversableOnce[U])(implicit arg0: Encoder[U]): Dataset[U]

(Scala-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results.
Experimental def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T]

(Java-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func.
Experimental def groupByKey[K](func: (T) ⇒ K)(implicit arg0: Encoder[K]): KeyValueGroupedDataset[K, T]

(Scala-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func.
def intersect(other: Dataset[T]): Dataset[T]

Returns a new Dataset containing rows only in both this Dataset and another Dataset.
Experimental def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)]

Using inner equi-join to join this Dataset returning a Tuple2 for each pair where condition evaluates to true.
Experimental def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)]

Joins this Dataset returning a Tuple2 for each pair where condition evaluates to true.
def limit(n: Int): Dataset[T]

Returns a new Dataset by taking the first n rows.
Experimental def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U]

(Java-specific) Returns a new Dataset that contains the result of applying func to each element.
Experimental def map[U](func: (T) ⇒ U)(implicit arg0: Encoder[U]): Dataset[U]

(Scala-specific) Returns a new Dataset that contains the result of applying func to each element.
Experimental def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U]

(Java-specific) Returns a new Dataset that contains the result of applying f to each partition.
Experimental def mapPartitions[U](func: (Iterator[T]) ⇒ Iterator[U])(implicit arg0: Encoder[U]): Dataset[U]

(Scala-specific) Returns a new Dataset that contains the result of applying func to each partition.
def orderBy(sortExprs: Column*): Dataset[T]

Returns a new Dataset sorted by the given expressions.
def orderBy(sortCol: String, sortCols: String*): Dataset[T]

Returns a new Dataset sorted by the given expressions.
def randomSplit(weights: Array[Double]): Array[Dataset[T]]

Randomly splits this Dataset with the provided weights.
def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]

Randomly splits this Dataset with the provided weights.
def randomSplitAsList(weights: Array[Double], seed: Long): List[Dataset[T]]

Returns a Java list that contains randomly split Dataset with the provided weights.
def repartition(partitionExprs: Column*): Dataset[T]

Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions.
def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]

Returns a new Dataset partitioned by the given partitioning expressions into numPartitions.
def repartition(numPartitions: Int): Dataset[T]

Returns a new Dataset that has exactly numPartitions partitions.
def sample(withReplacement: Boolean, fraction: Double): Dataset[T]

Returns a new Dataset by sampling a fraction of rows, using a random seed.
def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T]

Returns a new Dataset by sampling a fraction of rows, using a user-supplied seed.
Experimental def select[U1, U2, U3, U4, U5](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]

Returns a new Dataset by computing the given Column expressions for each element.
Experimental def select[U1, U2, U3, U4](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]

Returns a new Dataset by computing the given Column expressions for each element.
Experimental def select[U1, U2, U3](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]

Returns a new Dataset by computing the given Column expressions for each element.
Experimental def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]

Returns a new Dataset by computing the given Column expressions for each element.
Experimental def select[U1](c1: TypedColumn[T, U1]): Dataset[U1]

Returns a new Dataset by computing the given Column expression for each element.
def sort(sortExprs: Column*): Dataset[T]

Returns a new Dataset sorted by the given expressions.
def sort(sortCol: String, sortCols: String*): Dataset[T]

Returns a new Dataset sorted by the specified column, all in ascending order.
def sortWithinPartitions(sortExprs: Column*): Dataset[T]

Returns a new Dataset with each partition sorted by the given expressions.
def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T]

Returns a new Dataset with each partition sorted by the given expressions.
def transform[U](t: (Dataset[T]) ⇒ Dataset[U]): Dataset[U]

Concise syntax for chaining custom transformations.
def union(other: Dataset[T]): Dataset[T]

Returns a new Dataset containing union of rows in this Dataset and another Dataset.
def where(conditionExpr: String): Dataset[T]

Filters rows using the given SQL expression.
def where(condition: Column): Dataset[T]

Filters rows using the given condition.
def unionAll(other: Dataset[T]): Dataset[T]

Returns a new Dataset containing union of rows in this Dataset and another Dataset.

Untyped transformations

def agg(expr: Column, exprs: Column*): DataFrame

Aggregates on the entire Dataset without groups.
def agg(exprs: Map[String, String]): DataFrame

(Java-specific) Aggregates on the entire Dataset without groups.
def agg(exprs: Map[String, String]): DataFrame

(Scala-specific) Aggregates on the entire Dataset without groups.
def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame

(Scala-specific) Aggregates on the entire Dataset without groups.
def apply(colName: String): Column

Selects column based on the column name and return it as a Column.
def col(colName: String): Column

Selects column based on the column name and return it as a Column.
def crossJoin(right: Dataset[_]): DataFrame

Explicit cartesian join with another DataFrame.
def cube(col1: String, cols: String*): RelationalGroupedDataset

Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them.
def cube(cols: Column*): RelationalGroupedDataset

Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them.
def drop(col: Column): DataFrame

Returns a new Dataset with a column dropped.
def drop(colNames: String*): DataFrame

Returns a new Dataset with columns dropped.
def drop(colName: String): DataFrame

Returns a new Dataset with a column dropped.
def groupBy(col1: String, cols: String*): RelationalGroupedDataset

Groups the Dataset using the specified columns, so that we can run aggregation on them.
def groupBy(cols: Column*): RelationalGroupedDataset

Groups the Dataset using the specified columns, so we can run aggregation on them.
def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame

Join with another DataFrame, using the given join expression.
def join(right: Dataset[_], joinExprs: Column): DataFrame

Inner join with another DataFrame, using the given join expression.
def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame

Equi-join with another DataFrame using the given columns.
def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame

Inner equi-join with another DataFrame using the given columns.
def join(right: Dataset[_], usingColumn: String): DataFrame

Inner equi-join with another DataFrame using the given column.
def join(right: Dataset[_]): DataFrame

Join with another DataFrame.
def na: DataFrameNaFunctions

Returns a DataFrameNaFunctions for working with missing data.
def rollup(col1: String, cols: String*): RelationalGroupedDataset

Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them.
def rollup(cols: Column*): RelationalGroupedDataset

Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them.
def select(col: String, cols: String*): DataFrame

Selects a set of columns.
def select(cols: Column*): DataFrame

Selects a set of column based expressions.
def selectExpr(exprs: String*): DataFrame

Selects a set of SQL expressions.
def stat: DataFrameStatFunctions

Returns a DataFrameStatFunctions for working statistic functions support.
def withColumn(colName: String, col: Column): DataFrame

Returns a new Dataset by adding a column or replacing the existing column that has the same name.
def withColumnRenamed(existingName: String, newName: String): DataFrame

Returns a new Dataset with a column renamed.
def explode[A, B](inputColumn: String, outputColumn: String)(f: (A) ⇒ TraversableOnce[B])(implicit arg0: scala.reflect.api.JavaUniverse.TypeTag[B]): DataFrame

(Scala-specific) Returns a new Dataset where a single column has been expanded to zero or more rows by the provided function.
def explode[A <: Product](input: Column*)(f: (Row) ⇒ TraversableOnce[A])(implicit arg0: scala.reflect.api.JavaUniverse.TypeTag[A]): DataFrame

(Scala-specific) Returns a new Dataset where each row has been expanded to zero or more rows by the provided function.

Ungrouped

val queryExecution: QueryExecution
val sparkSession: SparkSession
lazy val sqlContext: SQLContext
def toJSON: Dataset[String]

Returns the content of the Dataset as a Dataset of JSON strings.
def toString(): String 
